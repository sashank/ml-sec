{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Setting up NLTK data for Naive Bayes...\")\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "print(\"âœ… Naive Bayes spam fighting setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Spam Classification with Kaggle Ling-Spam Dataset\n",
    "\n",
    "**About Naive Bayes for Spam Detection:**\n",
    "- Naive Bayes is one of the most effective methods for text classification\n",
    "- Assumes feature independence (naive assumption) but works well in practice\n",
    "- Uses probabilistic approach: P(spam|words) vs P(ham|words)\n",
    "- Fast training and prediction, suitable for large datasets\n",
    "\n",
    "**Dataset Setup:**\n",
    "1. Download the Ling-Spam dataset from Kaggle \n",
    "2. Place the CSV file in the 'datasets/' directory\n",
    "3. The CSV should have 'email_text' and 'label' columns\n",
    "4. If no dataset is found, a sample dataset will be created for demonstration\n",
    "\n",
    "**Feature Extraction Options:**\n",
    "- **Bag of Words (CountVectorizer)**: Simple word counts\n",
    "- **TF-IDF (TfidfVectorizer)**: Term frequency-inverse document frequency weighting\n",
    "- Both will be demonstrated for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Ling-Spam Dataset\n",
    "DATASET_PATH = 'datasets/lingspam_dataset.csv'\n",
    "TRAINING_SET_RATIO = 0.7\n",
    "\n",
    "# Text preprocessing parameters\n",
    "REMOVE_STOPWORDS = True\n",
    "USE_STEMMING = True\n",
    "MIN_DF = 2  # Minimum document frequency for features\n",
    "MAX_FEATURES = 5000  # Maximum number of features\n",
    "\n",
    "print(f\"ðŸ“ Dataset path: {DATASET_PATH}\")\n",
    "print(f\"ðŸ”„ Training ratio: {TRAINING_SET_RATIO}\")\n",
    "print(f\"ðŸ› ï¸  Remove stopwords: {REMOVE_STOPWORDS}\")\n",
    "print(f\"ðŸŒ± Use stemming: {USE_STEMMING}\")\n",
    "print(f\"ðŸ“Š Max features: {MAX_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lingspam_dataset():\n",
    "    \"\"\"\n",
    "    Load the Ling-Spam dataset from CSV format\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataset with email_text and label columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(DATASET_PATH):\n",
    "            df = pd.read_csv(DATASET_PATH)\n",
    "            print(f\"âœ… Loaded dataset from {DATASET_PATH}\")\n",
    "            print(f\"Dataset shape: {df.shape}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"âš ï¸  Dataset not found at {DATASET_PATH}\")\n",
    "            print(\"Creating sample dataset for Naive Bayes demonstration...\")\n",
    "            \n",
    "            # Create diverse sample dataset for Naive Bayes\n",
    "            sample_data = {\n",
    "                'email_text': [\n",
    "                    \"Dear colleague, I hope this email finds you well. We are organizing a linguistics conference next month on computational methods.\",\n",
    "                    \"URGENT!!! You have won $1,000,000!!! Click here now to claim your prize!!! Limited time offer!!! Don't miss out!!!\",\n",
    "                    \"The latest research on phonetics shows interesting patterns in vowel recognition systems and speech processing algorithms.\",\n",
    "                    \"FREE VIAGRA!!! Buy now with 90% discount!!! No prescription needed!!! Order today!!! Fast shipping worldwide!!!\",\n",
    "                    \"Thank you for your submission to the journal. We will review it and get back to you within 5-7 business days.\",\n",
    "                    \"MAKE MONEY FAST!!! Work from home!!! Earn $5000 per week!!! No experience required!!! Start immediately!!!\",\n",
    "                    \"The syntax paper you requested is attached. Please let me know if you need any clarifications on the methodology.\",\n",
    "                    \"CREDIT CARD DEBT FORGIVENESS!!! Eliminate your debt today!!! Government program!!! Call now for free consultation!!!\",\n",
    "                    \"Could you please review the manuscript on morphological analysis? Your expertise in computational linguistics would be valuable.\",\n",
    "                    \"WIN A FREE IPHONE!!! Click now!!! Limited time offer!!! Act fast!!! Only 100 phones left!!! Don't wait!!!\",\n",
    "                    \"The linguistics department is hosting a seminar on natural language processing next Friday at 2 PM in room A101.\",\n",
    "                    \"HOT SINGLES IN YOUR AREA!!! Meet them tonight!!! No strings attached!!! 100% free registration!!! Join now!!!\",\n",
    "                    \"I found your paper on semantic analysis very insightful. Would you be interested in collaboration on future research?\",\n",
    "                    \"LOSE 30 POUNDS IN 30 DAYS!!! Revolutionary diet pill!!! Doctor approved!!! No exercise needed!!! Order now!!!\",\n",
    "                    \"The conference proceedings are now available online. Thank you for your participation and excellent presentation.\",\n",
    "                    \"WORK FROM HOME!!! Earn $3000/week!!! No experience needed!!! Start today!!! Flexible hours!!! Apply now!!!\",\n",
    "                    \"Please find attached the corrected version of the phoneme classification algorithm for your review and comments.\",\n",
    "                    \"MIRACLE CURE!!! Lose weight without diet or exercise!!! 100% guaranteed!!! Revolutionary breakthrough!!! Order today!!!\",\n",
    "                    \"The workshop on computational linguistics has been scheduled for next month. We would appreciate your input.\",\n",
    "                    \"FREE MONEY!!! Government grants available!!! Claim yours now!!! No repayment required!!! Limited time offer!!!\"\n",
    "                ],\n",
    "                'label': ['ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam',\n",
    "                         'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam', 'ham', 'spam']\n",
    "            }\n",
    "            \n",
    "            df = pd.DataFrame(sample_data)\n",
    "            print(f\"Created sample dataset with {len(df)} emails\")\n",
    "            return df\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_lingspam_dataset()\n",
    "if dataset is not None:\n",
    "    print(f\"\\nðŸ“Š Dataset summary:\")\n",
    "    print(f\"Total emails: {len(dataset)}\")\n",
    "    print(f\"Label distribution:\")\n",
    "    print(dataset['label'].value_counts())\n",
    "    print(f\"\\nðŸ“ Sample emails:\")\n",
    "    print(f\"Ham: '{dataset[dataset['label']=='ham'].iloc[0]['email_text'][:80]}...'\")\n",
    "    print(f\"Spam: '{dataset[dataset['label']=='spam'].iloc[0]['email_text'][:80]}...'\")\n",
    "else:\n",
    "    print(\"âŒ Failed to load dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing for email classification\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw email text\n",
    "        \n",
    "    Returns:\n",
    "        str: Preprocessed text ready for vectorization\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, email addresses, and special characters\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "    except:\n",
    "        # Fallback to simple split if NLTK fails\n",
    "        tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    except:\n",
    "        print(\"âš ï¸  NLTK stopwords not available, skipping stopword removal\")\n",
    "        pass\n",
    "    \n",
    "    # Apply stemming\n",
    "    try:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    except:\n",
    "        print(\"âš ï¸  NLTK stemmer not available, skipping stemming\")\n",
    "        pass\n",
    "    \n",
    "    # Remove short tokens and join\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocessing and apply to dataset\n",
    "if dataset is not None:\n",
    "    print(\"ðŸ”§ Testing text preprocessing:\")\n",
    "    sample_text = dataset.iloc[0]['email_text']\n",
    "    print(f\"Original: '{sample_text[:100]}...'\")\n",
    "    \n",
    "    preprocessed = preprocess_text(sample_text)\n",
    "    print(f\"Preprocessed: '{preprocessed[:100]}...'\")\n",
    "    \n",
    "    # Apply preprocessing to entire dataset\n",
    "    print(f\"\\nâš™ï¸  Preprocessing all {len(dataset)} emails...\")\n",
    "    dataset['preprocessed_text'] = dataset['email_text'].apply(preprocess_text)\n",
    "    \n",
    "    # Remove empty emails after preprocessing\n",
    "    original_size = len(dataset)\n",
    "    dataset = dataset[dataset['preprocessed_text'].str.len() > 0]\n",
    "    if len(dataset) < original_size:\n",
    "        print(f\"Removed {original_size - len(dataset)} empty emails after preprocessing\")\n",
    "    \n",
    "    print(f\"âœ… Preprocessing complete! Dataset size: {len(dataset)} emails\")\n",
    "    print(f\"Sample preprocessed text: '{dataset.iloc[0]['preprocessed_text'][:80]}...'\")\n",
    "else:\n",
    "    print(\"âŒ No dataset available for preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and testing sets\n",
    "if dataset is not None:\n",
    "    print(\"ðŸ“Š Preparing train/test split...\")\n",
    "    \n",
    "    X = dataset['preprocessed_text']\n",
    "    y = dataset['label']\n",
    "    \n",
    "    # Split with stratification to maintain class balance\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(X_train)} emails ({len(y_train[y_train=='ham'])} ham, {len(y_train[y_train=='spam'])} spam)\")\n",
    "    print(f\"Test set: {len(X_test)} emails ({len(y_test[y_test=='ham'])} ham, {len(y_test[y_test=='spam'])} spam)\")\n",
    "    \n",
    "    # Verify no data leakage\n",
    "    train_texts = set(X_train)\n",
    "    test_texts = set(X_test)\n",
    "    overlap = train_texts.intersection(test_texts)\n",
    "    if len(overlap) > 0:\n",
    "        print(f\"âš ï¸  Warning: {len(overlap)} texts appear in both train and test sets\")\n",
    "    else:\n",
    "        print(\"âœ… No data leakage detected between train and test sets\")\n",
    "else:\n",
    "    print(\"âŒ No dataset available for splitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction with vectorization\n",
    "print(\"ðŸ”¤ Feature extraction with vectorization...\")\n",
    "\n",
    "if VECTORIZER_TYPE == 'count':\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=MAX_FEATURES,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),  # Use unigrams and bigrams\n",
    "        min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "        max_df=0.95  # Ignore terms that appear in more than 95% of documents\n",
    "    )\n",
    "    print(\"ðŸ“Š Using CountVectorizer (word counts)\")\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=MAX_FEATURES,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.95\n",
    "    )\n",
    "    print(\"ðŸ“Š Using TfidfVectorizer (TF-IDF scores)\")\n",
    "\n",
    "# Fit vectorizer on training data and transform both sets\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"âœ… Feature extraction complete!\")\n",
    "print(f\"Training features shape: {X_train_vectorized.shape}\")\n",
    "print(f\"Test features shape: {X_test_vectorized.shape}\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# Show some example features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"Sample features: {list(feature_names[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       Spam       0.99      0.94      0.97     15035\n",
      "        Ham       0.90      0.98      0.94      7591\n",
      "\n",
      "avg / total       0.96      0.96      0.96     22626\n",
      "\n",
      "Classification accuracy 95.6%\n"
     ]
    }
   ],
   "source": [
    "# Train Naive Bayes classifier\n",
    "print(\"ðŸ¤– Training Naive Bayes classifier...\")\n",
    "\n",
    "# Initialize the classifier\n",
    "nb_classifier = MultinomialNB(alpha=ALPHA)\n",
    "\n",
    "# Train the model\n",
    "nb_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = nb_classifier.predict(X_train_vectorized)\n",
    "y_test_pred = nb_classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_test_proba = nb_classifier.predict_proba(X_test_vectorized)\n",
    "\n",
    "print(\"âœ… Training complete!\")\n",
    "\n",
    "# Calculate accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Model Performance:\")\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Check for overfitting\n",
    "if train_accuracy - test_accuracy > 0.1:\n",
    "    print(\"âš ï¸  Warning: Possible overfitting detected (train accuracy >> test accuracy)\")\n",
    "else:\n",
    "    print(\"âœ… Good generalization (train and test accuracies are close)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation metrics\n",
    "print(\"ðŸ“Š Detailed Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Ham', 'Spam']))\n",
    "\n",
    "print(\"\\nðŸ”¢ Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_test_pred, labels=['ham', 'spam'])\n",
    "print(cm)\n",
    "\n",
    "# Calculate additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision_spam = tp / (tp + fp)\n",
    "recall_spam = tp / (tp + fn)\n",
    "f1_spam = 2 * (precision_spam * recall_spam) / (precision_spam + recall_spam)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Spam Detection Metrics:\")\n",
    "print(f\"Precision (Spam): {precision_spam:.4f} (of emails classified as spam, {precision_spam*100:.1f}% are actually spam)\")\n",
    "print(f\"Recall (Spam): {recall_spam:.4f} (of actual spam emails, {recall_spam*100:.1f}% are correctly identified)\")\n",
    "print(f\"F1-Score (Spam): {f1_spam:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ” Error Analysis:\")\n",
    "print(f\"False Positives (Ham classified as Spam): {fp}\")\n",
    "print(f\"False Negatives (Spam classified as Ham): {fn}\")\n",
    "\n",
    "if fp > 0:\n",
    "    print(\"âš ï¸  False positives are problematic - legitimate emails marked as spam\")\n",
    "if fn > 0:\n",
    "    print(\"âš ï¸  False negatives are concerning - spam emails reaching inbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature analysis - most informative features\n",
    "print(\"ðŸ” Most Informative Features for Spam Detection:\")\n",
    "\n",
    "# Get feature names and their log probabilities\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "log_prob_spam = nb_classifier.feature_log_prob_[1]  # Spam class\n",
    "log_prob_ham = nb_classifier.feature_log_prob_[0]   # Ham class\n",
    "\n",
    "# Calculate feature importance (log probability ratio)\n",
    "feature_importance = log_prob_spam - log_prob_ham\n",
    "\n",
    "# Get top features for spam\n",
    "spam_features_idx = np.argsort(feature_importance)[-20:][::-1]\n",
    "ham_features_idx = np.argsort(feature_importance)[:20]\n",
    "\n",
    "print(\"\\nðŸ“§ Top 10 Spam Indicators:\")\n",
    "for i, idx in enumerate(spam_features_idx[:10]):\n",
    "    feature = feature_names[idx]\n",
    "    importance = feature_importance[idx]\n",
    "    print(f\"{i+1:2d}. '{feature}' (score: {importance:.3f})\")\n",
    "\n",
    "print(\"\\nðŸ“® Top 10 Ham Indicators:\")\n",
    "for i, idx in enumerate(ham_features_idx[:10]):\n",
    "    feature = feature_names[idx]\n",
    "    importance = feature_importance[idx]\n",
    "    print(f\"{i+1:2d}. '{feature}' (score: {importance:.3f})\")\n",
    "\n",
    "# Show class probabilities\n",
    "print(f\"\\nðŸ“Š Class Distribution in Training Data:\")\n",
    "spam_prob = (y_train == 'spam').mean()\n",
    "ham_prob = (y_train == 'ham').mean()\n",
    "print(f\"Spam: {spam_prob:.3f} ({spam_prob*100:.1f}%)\")\n",
    "print(f\"Ham: {ham_prob:.3f} ({ham_prob*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction function\n",
    "def classify_email(email_text, show_probability=True):\n",
    "    \"\"\"\n",
    "    Classify a single email as spam or ham\n",
    "    \n",
    "    Args:\n",
    "        email_text (str): The email text to classify\n",
    "        show_probability (bool): Whether to show prediction probabilities\n",
    "    \n",
    "    Returns:\n",
    "        str: Classification result ('spam' or 'ham')\n",
    "    \"\"\"\n",
    "    # Preprocess the text\n",
    "    preprocessed = preprocess_text(email_text)\n",
    "    \n",
    "    if not preprocessed:\n",
    "        print(\"âš ï¸  Warning: Email text is empty after preprocessing\")\n",
    "        return None\n",
    "    \n",
    "    # Vectorize the text\n",
    "    vectorized = vectorizer.transform([preprocessed])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = nb_classifier.predict(vectorized)[0]\n",
    "    \n",
    "    if show_probability:\n",
    "        probabilities = nb_classifier.predict_proba(vectorized)[0]\n",
    "        ham_prob = probabilities[0] if nb_classifier.classes_[0] == 'ham' else probabilities[1]\n",
    "        spam_prob = probabilities[1] if nb_classifier.classes_[1] == 'spam' else probabilities[0]\n",
    "        \n",
    "        print(f\"ðŸ“§ Email Classification:\")\n",
    "        print(f\"Text: '{email_text[:100]}{'...' if len(email_text) > 100 else ''}'\")\n",
    "        print(f\"Prediction: {prediction.upper()}\")\n",
    "        print(f\"Confidence: Ham {ham_prob:.3f} | Spam {spam_prob:.3f}\")\n",
    "        \n",
    "        # Show confidence level\n",
    "        max_prob = max(ham_prob, spam_prob)\n",
    "        if max_prob > 0.9:\n",
    "            print(\"ðŸŸ¢ High confidence\")\n",
    "        elif max_prob > 0.7:\n",
    "            print(\"ðŸŸ¡ Medium confidence\")\n",
    "        else:\n",
    "            print(\"ðŸ”´ Low confidence\")\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Test with sample emails\n",
    "print(\"ðŸ§ª Testing classifier with sample emails:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_emails = [\n",
    "    \"Dear colleague, thank you for your research paper on computational linguistics.\",\n",
    "    \"URGENT!!! You have won $1,000,000!!! Click here now!!!\",\n",
    "    \"The conference paper deadline has been extended to next month.\",\n",
    "    \"FREE VIAGRA!!! Buy now with 90% discount!!! No prescription needed!!!\"\n",
    "]\n",
    "\n",
    "for i, email in enumerate(test_emails, 1):\n",
    "    print(f\"\\nTest {i}:\")\n",
    "    classify_email(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Confusion Matrix Heatmap\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# Plot 2: Class Distribution\n",
    "plt.subplot(2, 3, 2)\n",
    "class_counts = dataset['label'].value_counts()\n",
    "plt.pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%', \n",
    "        colors=['lightblue', 'lightcoral'])\n",
    "plt.title('Dataset Class Distribution')\n",
    "\n",
    "# Plot 3: Prediction Confidence Distribution\n",
    "plt.subplot(2, 3, 3)\n",
    "max_probabilities = np.max(y_test_proba, axis=1)\n",
    "plt.hist(max_probabilities, bins=20, alpha=0.7, color='green', edgecolor='black')\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction Confidence Distribution')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.7, label='50% threshold')\n",
    "plt.axvline(x=0.9, color='orange', linestyle='--', alpha=0.7, label='90% threshold')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 4: Feature Importance (Top Spam Features)\n",
    "plt.subplot(2, 3, 4)\n",
    "top_spam_features = feature_names[spam_features_idx[:10]]\n",
    "top_spam_scores = feature_importance[spam_features_idx[:10]]\n",
    "plt.barh(range(len(top_spam_features)), top_spam_scores, color='red', alpha=0.7)\n",
    "plt.yticks(range(len(top_spam_features)), top_spam_features)\n",
    "plt.xlabel('Log Probability Ratio')\n",
    "plt.title('Top 10 Spam Features')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Plot 5: Feature Importance (Top Ham Features)\n",
    "plt.subplot(2, 3, 5)\n",
    "top_ham_features = feature_names[ham_features_idx[:10]]\n",
    "top_ham_scores = feature_importance[ham_features_idx[:10]]\n",
    "plt.barh(range(len(top_ham_features)), top_ham_scores, color='blue', alpha=0.7)\n",
    "plt.yticks(range(len(top_ham_features)), top_ham_features)\n",
    "plt.xlabel('Log Probability Ratio')\n",
    "plt.title('Top 10 Ham Features')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Plot 6: Performance Metrics\n",
    "plt.subplot(2, 3, 6)\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "values = [test_accuracy, precision_spam, recall_spam, f1_spam]\n",
    "colors = ['blue', 'green', 'orange', 'red']\n",
    "plt.bar(metrics, values, color=colors, alpha=0.7)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics (Spam Class)')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/naive-bayes-analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Visualizations saved to 'figures/naive-bayes-analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "The Naive Bayes classifier demonstrates excellent performance for email spam detection:\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **High Accuracy**: The model achieves strong classification performance on the Ling-Spam dataset\n",
    "2. **Interpretable Features**: We can easily identify which words/phrases contribute most to spam/ham classification\n",
    "3. **Fast Training**: Naive Bayes trains quickly and requires minimal computational resources\n",
    "4. **Probabilistic Output**: The classifier provides confidence scores for predictions\n",
    "\n",
    "### Advantages of Naive Bayes for Spam Detection:\n",
    "\n",
    "- **Simple and Fast**: Easy to implement and train\n",
    "- **Handles Text Well**: Natural fit for bag-of-words text classification\n",
    "- **Feature Independence**: Works well even when word independence assumption is violated\n",
    "- **Probabilistic**: Provides meaningful confidence scores\n",
    "- **Small Dataset Friendly**: Performs well with limited training data\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- **Feature Independence Assumption**: Assumes words are independent (not always true)\n",
    "- **Zero Frequency Problem**: Smoothing (alpha parameter) needed for unseen words\n",
    "- **Context Ignorance**: Doesn't consider word order or context\n",
    "\n",
    "### Security Applications:\n",
    "\n",
    "This approach can be extended to:\n",
    "- Email spam filtering systems\n",
    "- Malicious URL detection\n",
    "- Phishing email identification\n",
    "- Content-based security filtering\n",
    "- Social media spam detection\n",
    "\n",
    "The combination of high accuracy, interpretability, and speed makes Naive Bayes an excellent baseline for text-based security applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
